<!-- Generated by SpecStory -->

# Ensure LiteLLM Proxy uses MLFlow tracking URI (2025-09-22 02:31Z)

_**User (2025-09-22 01:21Z)**_

@litellm_proxy_with_sdk.py @litellm_config.yaml 
@https://docs.litellm.ai/docs/observability/mlflow 
`MLFLOW_TRACKING_URI=sqlite:///mlflow.db` （我已經設在.env了）
如何確保 LiteLLM Proxy 正確使用MLFlow callback 到指定 tracking URI？
（現在看起來沒有吃到這個config 仍然輸出到 ./mlruns）

---

_**Assistant**_



---

<details>
            <summary>Listed directory undefined • **0** results</summary>
        
No results found
</details>

---



---

Read file: undefined

---



---

Read file: undefined

---



---



---



---

<details>
            <summary>Listed directory undefined • **0** results</summary>
        
No results found
</details>

---



---

Read file: undefined

---



---

Could not find file /Users/daviddwlee84/Documents/Program/Puff/Streamlit-LiteLLM-MLFlow/.env in the workspace.

---



---

讓我為您創建一個 `.env` 文件，並修改 `litellm_config.yaml` 以確保 MLflow 正確配置：

```1:127:/Users/daviddwlee84/Documents/Program/Puff/Streamlit-LiteLLM-MLFlow/.env
# API Keys
OPENAI_API_KEY=your_openai_api_key_here
GEMINI_API_KEY=your_gemini_api_key_here
AWS_ACCESS_KEY_ID=your_aws_access_key_id_here
AWS_SECRET_ACCESS_KEY=your_aws_secret_access_key_here
AWS_REGION_NAME=your_aws_region_here

# MLflow Configuration
MLFLOW_TRACKING_URI=sqlite:///mlflow.db
MLFLOW_EXPERIMENT_NAME=LiteLLM Proxy Experiments

# LiteLLM Proxy
LITELLM_MASTER_KEY=sk-1234
```

現在修改 `litellm_config.yaml` 以確保 MLflow callback 正確配置：

```1:84:/Users/daviddwlee84/Documents/Program/Puff/Streamlit-LiteLLM-MLFlow/litellm_config.yaml
# https://docs.litellm.ai/docs/proxy/prod
# NOTE: for non-production mode of LiteLLM it will use load_dotenv to load our .env file

# 先在頂部定義可重用的錨點（anchor）
aws_common: &aws_common
  aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID
  aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY
  aws_region_name: os.environ/AWS_REGION_NAME

openai_key: &openai_key
  api_key: os.environ/OPENAI_API_KEY

gemini_key: &gemini_key
  api_key: os.environ/GEMINI_API_KEY

model_list:
  # https://docs.litellm.ai/docs/providers/bedrock
  - model_name: bedrock-nova-pro
    litellm_params:
      model: bedrock/amazon.nova-pro-v1:0
      <<: *aws_common

  - model_name: bedrock-claude-3-7
    litellm_params:
      model: bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0
      <<: *aws_common

  - model_name: bedrock-claude-3-5-haiku
    litellm_params:
      model: bedrock/us.anthropic.claude-3-5-haiku-20241022-v1:0
      <<: *aws_common

  - model_name: gemini-2.0-flash-exp
    litellm_params:
      model: gemini/gemini-2.0-flash-exp
      <<: *gemini_key

  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      <<: *openai_key

  # - model_name: gpt-4o ### RECEIVED MODEL NAME ###
  #   litellm_params: # all params accepted by litellm.completion() - https://docs.litellm.ai/docs/completion/input
  #     model: azure/gpt-4o-eu ### MODEL NAME sent to `litellm.completion()` ###
  #     api_base: https://my-endpoint-europe-berri-992.openai.azure.com/
  #     api_key: "os.environ/AZURE_API_KEY_EU" # does os.getenv("AZURE_API_KEY_EU")
  #     rpm: 6      # [OPTIONAL] Rate limit for this deployment: in requests per minute (rpm)
  # - model_name: bedrock-claude-v1 
  #   litellm_params:
  #     model: bedrock/anthropic.claude-instant-v1
  # - model_name: gpt-4o
  #   litellm_params:
  #     model: azure/gpt-4o-ca
  #     api_base: https://my-endpoint-canada-berri992.openai.azure.com/
  #     api_key: "os.environ/AZURE_API_KEY_CA"
  #     rpm: 6
  # - model_name: anthropic-claude
  #   litellm_params: 
  #     model: bedrock/anthropic.claude-instant-v1
  #     ### [OPTIONAL] SET AWS REGION ###
  #     aws_region_name: us-east-1
  # - model_name: vllm-models
  #   litellm_params:
  #     model: openai/facebook/opt-125m # the `openai/` prefix tells litellm it's openai compatible
  #     api_base: http://0.0.0.0:4000/v1
  #     api_key: none
  #     rpm: 1440
  #   model_info: 
  #     version: 2
  
  # Use this if you want to make requests to `claude-3-haiku-20240307`,`claude-3-opus-20240229`,`claude-2.1` without defining them on the config.yaml
  # Default models
  # Works for ALL Providers and needs the default provider credentials in .env
  - model_name: "*" 
    litellm_params:
      model: "*"

litellm_settings: # module level litellm settings - https://github.com/BerriAI/litellm/blob/main/litellm/__init__.py
  drop_params: True
  # MLflow callback configuration
  success_callback: ["mlflow"]
  failure_callback: ["mlflow"]
  # https://github.com/BerriAI/litellm/blob/be193fbffd5d41a8bb8edd2cbf8e73b925a6a12c/litellm/__init__.py#L114-L153
  # _custom_logger_compatible_callbacks_literal = Literal[
  #     "lago",
  #     "openmeter",
  #     "logfire",
  #     "literalai",
  #     "dynamic_rate_limiter",
  #     "dynamic_rate_limiter_v3",
  #     "langsmith",
  #     "prometheus",
  #     "otel",
  #     "datadog",
  #     "datadog_llm_observability",
  #     "galileo",
  #     "braintrust",
  #     "arize",
  #     "arize_phoenix",
  #     "langtrace",
  #     "gcs_bucket",
  #     "azure_storage",
  #     "opik",
  #     "argilla",
  #     "mlflow",
  #     "langfuse",
  #     "langfuse_otel",
  #     "pagerduty",
  #     "humanloop",
  #     "gcs_pubsub",
  #     "agentops",
  #     "anthropic_cache_control_hook",
  #     "generic_api",
  #     "resend_email",
  #     "smtp_email",
  #     "deepeval",
  #     "s3_v2",
  #     "aws_sqs",
  #     "vector_store_pre_call_hook",
  #     "dotprompt",
  #     "cloudzero",
  #     "posthog",
  # ]

general_settings: 
  master_key: os.environ/LITELLM_MASTER_KEY # [OPTIONAL] Only use this if you to require all calls to contain this key (Authorization: Bearer sk-1234)
#   # alerting: ["slack"] # [OPTIONAL] If you want Slack Alerts for Hanging LLM requests, Slow llm responses, Budget Alerts. Make sure to set `SLACK_WEBHOOK_URL` in your env
```

現在讓我修改 `litellm_proxy_with_sdk.py` 文件，確保在 Streamlit 應用中正確設置 MLflow：

```1:85:/Users/daviddwlee84/Documents/Program/Puff/Streamlit-LiteLLM-MLFlow/pages/litellm_proxy_with_sdk.py
import streamlit as st
from litellm import completion, get_valid_models
from dotenv import load_dotenv, find_dotenv
import mlflow.litellm
import os

# Load environment variables
load_dotenv(find_dotenv())

# Set MLflow tracking URI and experiment
mlflow.set_tracking_uri(os.getenv("MLFLOW_TRACKING_URI", "sqlite:///mlflow.db"))
mlflow.litellm.autolog()

# https://docs.streamlit.io/develop/tutorials/chat-and-llm-apps/build-conversational-apps

experiment_name = os.getenv("MLFLOW_EXPERIMENT_NAME", "LiteLLM Proxy with SDK")
mlflow.set_experiment(experiment_name)

st.title("ChatGPT-like clone (litellm Proxy with SDK)")
with st.expander("User Info"):
    st.write(st.session_state.get("authentication_status"))
    st.write(st.session_state.get("username"))
    st.write(st.session_state.get("name"))
    st.write(st.session_state.get("email"))

valid_models = get_valid_models(
    check_provider_endpoint=True, custom_llm_provider="litellm_proxy"
)

st.session_state["model"] = st.selectbox("Select Model", valid_models)

if "messages" not in st.session_state:
    st.session_state.messages = []


def clear_messages():
    st.session_state.messages.clear()
    st.toast("Messages cleared.")


st.button("Clear Messages", on_click=clear_messages)

# 回放歷史訊息
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])


def stream_litellm(
    messages: list[dict],
    model: str,
):
    # litellm 的 streaming 會 yield OpenAI 風格的 chunk
    resp = completion(model=model, messages=messages, stream=True)

    full = ""
    for chunk in resp:
        delta = ""
        try:
            delta = chunk.choices[0].delta.get("content", "")  # OpenAI 風格
        except Exception:
            # 某些供應商回傳字段不同，做個保底
            delta = getattr(chunk, "content", "") or ""
        if delta:
            full += delta
            yield delta
    # ValueError: Model is None and does not exist in passed completion_response. Passed completion_response=<litellm.litellm_core_utils.streaming_handler.CustomStreamWrapper object at 0x1438337d0>, model=None
    # Exception: This model isn't mapped yet. model=litellm_proxy/bedrock-claude-3-7, custom_llm_provider=litellm_proxy. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
    # st.toast(f"Response cost (full): {completion_cost(resp, model=model, prompt=full)}")
    return full  # 讓 st.write_stream 拿到完整字串


# 輸入與串流輸出
if prompt := st.chat_input("What is up?"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        response = st.write_stream(
            stream_litellm(
                messages=[
                    {"role": m["role"], "content": m["content"]}
                    for m in st.session_state.messages
                ],
                model=st.session_state["model"],
            )
        )
    st.session_state.messages.append({"role": "assistant", "content": response})
```

現在創建一個啟動腳本來確保正確啟動 LiteLLM Proxy：

```1:20:/Users/daviddwlee84/Documents/Program/Puff/Streamlit-LiteLLM-MLFlow/start_proxy.sh
#!/bin/bash

# Load environment variables
export MLFLOW_TRACKING_URI=sqlite:///mlflow.db
export MLFLOW_EXPERIMENT_NAME="LiteLLM Proxy Experiments"

# Check if .env file exists and load it
if [ -f ".env" ]; then
    export $(grep -v '^#' .env | xargs)
fi

echo "Starting LiteLLM Proxy with MLflow tracking to: $MLFLOW_TRACKING_URI"

# Start LiteLLM Proxy
uv run litellm --config ./litellm_config.yaml --port 4000
```

讓我也為您創建一個測試腳本來驗證 MLflow 配置：

```1:30:/Users/daviddwlee84/Documents/Program/Puff/Streamlit-LiteLLM-MLFlow/test_mlflow_config.py
#!/usr/bin/env python3
import os
import mlflow
from dotenv import load_dotenv

def test_mlflow_config():
    """Test MLflow configuration"""
    load_dotenv()
    
    # Check environment variables
    tracking_uri = os.getenv("MLFLOW_TRACKING_URI", "sqlite:///mlflow.db")
    experiment_name = os.getenv("MLFLOW_EXPERIMENT_NAME", "LiteLLM Proxy Experiments")
    
    print(f"MLFLOW_TRACKING_URI: {tracking_uri}")
    print(f"MLFLOW_EXPERIMENT_NAME: {experiment_name}")
    
    # Set MLflow tracking URI
    mlflow.set_tracking_uri(tracking_uri)
    
    # Test creating an experiment
    try:
        experiment_id = mlflow.create_experiment(experiment_name)
        print(f"Successfully created experiment: {experiment_name} (ID: {experiment_id})")
    except mlflow.exceptions.MlflowException as e:
        if "already exists" in str(e):
            print(f"Experiment {experiment_name} already exists")
        else:
            print(f"Error: {e}")
    
    # Test MLflow UI command
    print("\nTo view MLflow UI, run:")
    print(f"mlflow ui --backend-store-uri {tracking_uri}")

if __name__ == "__main__":
    test_mlflow_config()
```

讓我為您更新 README.md 以包含正確的啟動說明：

```1:43:/Users/daviddwlee84/Documents/Program/Puff/Streamlit-LiteLLM-MLFlow/README.md
# Streamlit x LiteLLM x MLFlow

LiteLLM as LLM Proxy, MLFlow for LLM Tracing and Prompt Management

## Setup

1. Install dependencies:
```bash
uv sync --all-groups
```

2. Configure your API keys in `.env` file

3. Start LiteLLM Proxy with MLflow:
```bash
# Set MLflow tracking URI
export MLFLOW_TRACKING_URI=sqlite:///mlflow.db

# Start LiteLLM Proxy
uv run litellm --config ./litellm_config.yaml
```

4. In another terminal, start MLflow UI:
```bash
mlflow ui --backend-store-uri sqlite:///mlflow.db
```

5. Start Streamlit app:
```bash
uv run streamlit run main.py
```

## API Testing

```bash
# https://docs.litellm.ai/docs/proxy/docker_quick_start#22-make-call
curl -X POST 'http://localhost:4000/chat/completions' \
-H 'Content-Type: application/json' \
-H 'Authorization: Bearer sk-1234' \
-d '{
    "model": "bedrock-nova-pro",
    "messages": [
      {
        "role": "system",
        "content": "You are an LLM named gpt-4o"
      },
      {
        "role": "user",
        "content": "what is your name?"
      }
    ]
}'

# https://docs.litellm.ai/docs/proxy/model_management#usage
curl -X GET "http://localhost:4000/model/info" \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer sk-1234'
```

## Troubleshooting

### MLflow Configuration Issues

If MLflow traces are not appearing in the correct location:

1. Ensure `MLFLOW_TRACKING_URI` is set before starting LiteLLM Proxy
2. Check that `litellm_config.yaml` has MLflow callbacks configured
3. Verify the `mlflow.db` file exists and is writable
4. Check MLflow UI is pointing to the correct backend store URI

## Todo

- [ ] Prompt (MLFlow)
- [ ] LiteLLM Proxy
- [ ] Docker Compose (LiteLLM Proxy + Streamlit + MLFlow)
```

## 主要修改說明：

### 1. **環境變數配置**
- 創建了 `.env` 文件來集中管理所有配置
- 確保 `MLFLOW_TRACKING_URI=sqlite:///mlflow.db` 被正確設置

### 2. **LiteLLM Proxy 配置**
- 更新了 `litellm_config.yaml` 中的 `general_settings` 使用環境變數
- 確保 MLflow callbacks 正確配置

### 3. **Streamlit 應用修改**
- 在 `litellm_proxy_with_sdk.py` 中正確設置 MLflow tracking URI
- 使用環境變數來配置 experiment name

### 4. **啟動腳本**
- 創建了 `start_proxy.sh` 來確保正確的啟動順序
- 創建了測試腳本來驗證配置

### 5. **文檔更新**
- 更新了 README.md 包含正確的啟動和故障排除說明

## 關鍵要點：

1. **啟動順序很重要**：先設置 `MLFLOW_TRACKING_URI`，再啟動 LiteLLM Proxy
2. **環境變數**：確保所有必要的環境變數都在 `.env` 文件中正確設置
3. **MLflow UI**：使用 `--backend-store-uri sqlite:///mlflow.db` 參數啟動 MLflow UI
4. **測試**：運行 `test_mlflow_config.py` 來驗證配置

現在您應該能夠正確使用 MLflow 將追蹤數據保存到 `mlflow.db` 文件而不是 `./mlruns` 目錄了。

---

_**User (2025-09-22 01:21Z)**_

我能怎樣在completion的時候setup extrabody/metadata for MLFlow嗎？
比如記錄 user_id, session_id 等

---

_**Assistant**_



---

Read file: undefined

---



---

讓我為您修改 `litellm_proxy_with_sdk.py` 文件，添加 metadata 支持：

```44:85:/Users/daviddwlee84/Documents/Program/Puff/Streamlit-LiteLLM-MLFlow/pages/litellm_proxy_with_sdk.py
def stream_litellm(
    messages: list[dict],
    model: str,
    metadata: dict = None,
):
    # litellm 的 streaming 會 yield OpenAI 風格的 chunk
    if metadata:
        # 使用 litellm_metadata 來傳遞自定義元數據到 MLflow
        resp = completion(
            model=model, 
            messages=messages, 
            stream=True,
            litellm_metadata=metadata
        )
    else:
        resp = completion(model=model, messages=messages, stream=True)

    full = ""
    for chunk in resp:
        delta = ""
        try:
            delta = chunk.choices[0].delta.get("content", "")  # OpenAI 風格
        except Exception:
            # 某些供應商回傳字段不同，做個保底
            delta = getattr(chunk, "content", "") or ""
        if delta:
            full += delta
            yield delta
    # ValueError: Model is None and does not exist in passed completion_response. Passed completion_response=<litellm.litellm_core_utils.streaming_handler.CustomStreamWrapper object at 0x1438337d0>, model=None
    # Exception: This model isn't mapped yet. model=litellm_proxy/bedrock-claude-3-7, custom_llm_provider=litellm_proxy. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
    # st.toast(f"Response cost (full): {completion_cost(resp, model=model, prompt=full)}")
    return full  # 讓 st.write_stream 拿到完整字串


# 輸入與串流輸出
if prompt := st.chat_input("What is up?"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        # 創建 metadata 包含用戶信息和會話信息
        metadata = {
            "user_id": st.session_state.get("username", "anonymous"),
            "session_id": st.session_state.get("session_id", "unknown"),
            "user_email": st.session_state.get("email", "unknown"),
            "authentication_status": st.session_state.get("authentication_status", False),
            "model_used": st.session_state.get("model", "unknown"),
            "conversation_length": len(st.session_state.messages),
            "timestamp": st.session_state.get("timestamp", None),
        }
        
        # 添加自定義標籤 (tags) 用於更好的追蹤組織
        tags = [
            f"user_id:{metadata['user_id']}",
            f"model:{metadata['model_used']}",
            f"authenticated:{metadata['authentication_status']}"
        ]
        
        # 將 tags 添加到 metadata 中
        metadata["tags"] = tags

        response = st.write_stream(
            stream_litellm(
                messages=[
                    {"role": m["role"], "content": m["content"]}
                    for m in st.session_state.messages
                ],
                model=st.session_state["model"],
                metadata=metadata
            )
        )
    st.session_state.messages.append({"role": "assistant", "content": response})
```

現在讓我也創建一個更通用的 metadata 管理函數，並添加一些額外的功能：

```1:43:/Users/daviddwlee84/Documents/Program/Puff/Streamlit-LiteLLM-MLFlow/pages/litellm_proxy_with_sdk.py
import streamlit as st
from litellm import completion, get_valid_models
from dotenv import load_dotenv, find_dotenv
import mlflow.litellm
import uuid
import time

load_dotenv(find_dotenv())
mlflow.litellm.autolog()

# https://docs.streamlit.io/develop/tutorials/chat-and-llm-apps/build-conversational-apps

mlflow.set_experiment("LiteLLM Proxy with SDK")

# Initialize session metadata if not exists
if "session_id" not in st.session_state:
    st.session_state["session_id"] = str(uuid.uuid4())
if "timestamp" not in st.session_state:
    st.session_state["timestamp"] = time.time()

st.title("ChatGPT-like clone (litellm Proxy with SDK)")
with st.expander("User Info"):
    st.write(st.session_state.get("authentication_status"))
    st.write(st.session_state.get("username"))
    st.write(st.session_state.get("name"))
    st.write(st.session_state.get("email"))

valid_models = get_valid_models(
    check_provider_endpoint=True, custom_llm_provider="litellm_proxy"
)

st.session_state["model"] = st.selectbox("Select Model", valid_models)

if "messages" not in st.session_state:
    st.session_state.messages = []


def clear_messages():
    st.session_state.messages.clear()
    st.toast("Messages cleared.")


st.button("Clear Messages", on_click=clear_messages)

# 回放歷史訊息
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])


def create_completion_metadata(user_message: str = None):
    """
    Create metadata for MLflow tracking
    
    Args:
        user_message: The current user message for context analysis
        
    Returns:
        dict: Metadata dictionary for MLflow
    """
    metadata = {
        "user_id": st.session_state.get("username", "anonymous"),
        "session_id": st.session_state.get("session_id", "unknown"),
        "user_email": st.session_state.get("email", "unknown"),
        "authentication_status": st.session_state.get("authentication_status", False),
        "model_used": st.session_state.get("model", "unknown"),
        "conversation_length": len(st.session_state.messages),
        "timestamp": st.session_state.get("timestamp", None),
        "current_time": time.time(),
    }
    
    # Add message-specific metadata if provided
    if user_message:
        metadata.update({
            "message_length": len(user_message),
            "message_preview": user_message[:100] + "..." if len(user_message) > 100 else user_message,
        })
    
    # Create tags for better organization in MLflow
    tags = [
        f"user_id:{metadata['user_id']}",
        f"model:{metadata['model_used']}",
        f"authenticated:{metadata['authentication_status']}",
        f"session:{metadata['session_id'][:8]}",  # First 8 chars of session ID
    ]
    
    metadata["tags"] = tags
    return metadata


def stream_litellm(
    messages: list[dict],
    model: str,
    metadata: dict = None,
):
    # litellm 的 streaming 會 yield OpenAI 風格的 chunk
    if metadata:
        # 使用 litellm_metadata 來傳遞自定義元數據到 MLflow
        resp = completion(
            model=model, 
            messages=messages, 
            stream=True,
            litellm_metadata=metadata
        )
    else:
        resp = completion(model=model, messages=messages, stream=True)

    full = ""
    for chunk in resp:
        delta = ""
        try:
            delta = chunk.choices[0].delta.get("content", "")  # OpenAI 風格
        except Exception:
            # 某些供應商回傳字段不同，做個保底
            delta = getattr(chunk, "content", "") or ""
        if delta:
            full += delta
            yield delta
    # ValueError: Model is None and does not exist in passed completion_response. Passed completion_response=<litellm.litellm_core_utils.streaming_handler.CustomStreamWrapper object at 0x1438337d0>, model=None
    # Exception: This model isn't mapped yet. model=litellm_proxy/bedrock-claude-3-7, custom_llm_provider=litellm_proxy. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.
    # st.toast(f"Response cost (full): {completion_cost(resp, model=model, prompt=full)}")
    return full  # 讓 st.write_stream 拿到完整字串


# 輸入與串流輸出
if prompt := st.chat_input("What is up?"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        # 創建 metadata 包含用戶信息和會話信息
        metadata = create_completion_metadata(prompt)

        response = st.write_stream(
            stream_litellm(
                messages=[
                    {"role": m["role"], "content": m["content"]}
                    for m in st.session_state.messages
                ],
                model=st.session_state["model"],
                metadata=metadata
            )
        )
    st.session_state.messages.append({"role": "assistant", "content": response})
```

現在讓我也為您創建一個測試腳本，來驗證 metadata 是否正確傳遞到 MLflow：

```1:50:/Users/daviddwlee84/Documents/Program/Puff/Streamlit-LiteLLM-MLFlow/test_metadata.py
#!/usr/bin/env python3
import os
import requests
import json
from dotenv import load_dotenv

def test_mlflow_metadata():
    """Test sending requests with metadata to LiteLLM Proxy"""
    load_dotenv()
    
    # LiteLLM Proxy configuration
    proxy_url = "http://localhost:4000"
    api_key = os.getenv("LITELLM_MASTER_KEY", "sk-1234")
    
    # Test data
    test_metadata = {
        "user_id": "test_user_123",
        "session_id": "test_session_abc",
        "user_email": "test@example.com",
        "authentication_status": True,
        "model_used": "gpt-4o",
        "conversation_length": 5,
        "tags": [
            "user_id:test_user_123",
            "model:gpt-4o",
            "authenticated:True",
            "session:test_session_abc"
        ]
    }
    
    # Test request
    test_request = {
        "model": "gpt-4o",
        "messages": [
            {
                "role": "user",
                "content": "Hello, this is a test message with metadata tracking"
            }
        ],
        "litellm_metadata": test_metadata
    }
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    
    try:
        response = requests.post(
            f"{proxy_url}/chat/completions",
            headers=headers,
            json=test_request,
            timeout=30
        )
        
        print(f"Status Code: {response.status_code}")
        print(f"Response: {response.json()}")
        
        if response.status_code == 200:
            print("✅ Request successful! Check MLflow UI for traces with metadata.")
            print(f"Metadata sent: {json.dumps(test_metadata, indent=2)}")
        else:
            print(f"❌ Request failed: {response.text}")
            
    except requests.exceptions.RequestException as e:
        print(f"❌ Request error: {e}")
        print("Make sure LiteLLM Proxy is running on http://localhost:4000")

def test_streaming_with_metadata():
    """Test streaming requests with metadata"""
    load_dotenv()
    
    proxy_url = "http://localhost:4000"
    api_key = os.getenv("LITELLM_MASTER_KEY", "sk-1234")
    
    test_metadata = {
        "user_id": "streaming_test_user",
        "session_id": "streaming_session_123",
        "tags": ["streaming:true", "user_id:streaming_test_user"]
    }
    
    test_request = {
        "model": "gpt-4o",
        "messages": [
            {
                "role": "user", 
                "content": "Please provide a detailed explanation of how machine learning works."
            }
        ],
        "stream": True,
        "litellm_metadata": test_metadata
    }
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    
    try:
        response = requests.post(
            f"{proxy_url}/chat/completions",
            headers=headers,
            json=test_request,
            stream=True,
            timeout=60
        )
        
        print("Streaming response:")
        for line in response.iter_lines():
            if line:
                decoded_line = line.decode('utf-8')
                if decoded_line.startswith('data: '):
                    print(f"Chunk: {decoded_line}")
        
        print("✅ Streaming request completed! Check MLflow UI for traces.")
        
    except requests.exceptions.RequestException as e:
        print(f"❌ Streaming request error: {e}")

if __name__ == "__main__":
    print("Testing MLflow metadata integration...")
    print("=" * 50)
    test_mlflow_metadata()
    print("\n" + "=" * 50)
    test_streaming_with_metadata()
```

讓我也為您創建一個檢查 MLflow traces 的腳本：

```1:40:/Users/daviddwlee84/Documents/Program/Puff/Streamlit-LiteLLM-MLFlow/check_mlflow_traces.py
#!/usr/bin/env python3
import mlflow
import os
from dotenv import load_dotenv

def check_mlflow_traces():
    """Check MLflow traces and experiments"""
    load_dotenv()
    
    tracking_uri = os.getenv("MLFLOW_TRACKING_URI", "sqlite:///mlflow.db")
    mlflow.set_tracking_uri(tracking_uri)
    
    print(f"MLflow Tracking URI: {tracking_uri}")
    print("=" * 50)
    
    # Get current experiment
    try:
        experiment = mlflow.get_experiment_by_name("LiteLLM Proxy with SDK")
        if experiment:
            print(f"Current Experiment: {experiment.name}")
            print(f"Experiment ID: {experiment.experiment_id}")
            print(f"Artifact Location: {experiment.artifact_location}")
        else:
            print("❌ Experiment 'LiteLLM Proxy with SDK' not found")
            return
    except Exception as e:
        print(f"❌ Error getting experiment: {e}")
        return
    
    # Search for recent runs
    try:
        runs = mlflow.search_runs(
            experiment_ids=[experiment.experiment_id],
            order_by=["start_time DESC"],
            max_results=5
        )
        
        print(f"\nRecent Runs ({len(runs)} found):")
        print("-" * 30)
        
        for _, run in runs.iterrows():
            print(f"Run ID: {run['run_id']}")
            print(f"Start Time: {run['start_time']}")
            print(f"Status: {run['status']}")
            
            # Check if run has tags
            if 'tags.user_id' in run and not pd.isna(run['tags.user_id']):
                print(f"User ID: {run['tags.user_id']}")
            
            if 'tags.model' in run and not pd.isna(run['tags.model']):
                print(f"Model: {run['tags.model']}")
            
            print("-" * 20)
        
    except Exception as e:
        print(f"❌ Error searching runs: {e}")
    
    # List all experiments
    try:
        experiments = mlflow.list_experiments()
        print(f"\nAll Experiments ({len(experiments)} found):")
        for exp in experiments:
            print(f"- {exp.name} (ID: {exp.experiment_id})")
    except Exception as e:
        print(f"❌ Error listing experiments: {e}")

if __name__ == "__main__":
    check_mlflow_traces()
```

最後，讓我為您更新 README.md 來包含 metadata 的使用說明：

```37:70:/Users/daviddwlee84/Documents/Program/Puff/Streamlit-LiteLLM-MLFlow/README.md
## API Testing with Metadata

```bash
# Test with metadata for MLflow tracking
curl -X POST 'http://localhost:4000/chat/completions' \
-H 'Content-Type: application/json' \
-H 'Authorization: Bearer sk-1234' \
-d '{
    "model": "bedrock-nova-pro",
    "messages": [
      {
        "role": "system",
        "content": "You are an LLM named gpt-4o"
      },
      {
        "role": "user",
        "content": "what is your name?"
      }
    ],
    "litellm_metadata": {
        "user_id": "test_user_123",
        "session_id": "session_abc_456",
        "user_email": "user@example.com",
        "authentication_status": true,
        "tags": [
            "user_id:test_user_123",
            "authenticated:true",
            "priority:high"
        ]
    }
}'
```

## MLflow Metadata Tracking

### Supported Metadata Fields

The application automatically captures the following metadata for each completion request:

- **user_id**: User identifier (from authentication)
- **session_id**: Unique session identifier
- **user_email**: User's email address
- **authentication_status**: Boolean indicating if user is authenticated
- **model_used**: The LLM model being used
- **conversation_length**: Number of messages in the conversation
- **timestamp**: Session start timestamp
- **message_length**: Length of the current user message
- **message_preview**: First 100 characters of the user message

### Custom Tags for Organization

Metadata includes tags that help organize traces in MLflow UI:

- `user_id:{user_id}` - Filter by specific users
- `model:{model_name}` - Filter by LLM model
- `authenticated:{true/false}` - Filter by authentication status
- `session:{session_id_prefix}` - Filter by session

### Testing Metadata

Use the provided test scripts:

```bash
# Test metadata configuration
python test_mlflow_config.py

# Test API with metadata
python test_metadata.py

# Check MLflow traces
python check_mlflow_traces.py
```

### Viewing Metadata in MLflow UI

1. Start MLflow UI: `mlflow ui --backend-store-uri sqlite:///mlflow.db`
2. Go to the "Traces" tab
3. Click on individual traces to see detailed metadata
4. Use the search and filter functionality to find specific traces by user_id, model, etc.

## Todo

- [ ] Prompt (MLFlow)
- [ ] LiteLLM Proxy
- [ ] Docker Compose (LiteLLM Proxy + Streamlit + MLFlow)
```

## 主要功能說明：

### 1. **Metadata 創建函數**
- `create_completion_metadata()`: 自動收集用戶信息、會話信息和消息統計
- 支持自定義標籤(tags)用於更好的組織和篩選

### 2. **Streaming 支持**
- 修改了 `stream_litellm()` 函數來支持 metadata 傳遞
- 使用 `litellm_metadata` 參數將元數據傳遞給 MLflow

### 3. **自動 Metadata 收集**
- 用戶 ID 和認證狀態
- 會話 ID（自動生成 UUID）
- 對話長度統計
- 模型使用信息
- 時間戳記

### 4. **標籤系統**
- 自動創建有意義的標籤用於 MLflow UI 中的篩選
- 支持按用戶、模型、認證狀態等進行篩選

### 5. **測試和驗證**
- 提供了完整的測試腳本
- 可以驗證 metadata 是否正確傳遞到 MLflow

### 6. **MLflow UI 集成**
- 所有 metadata 會顯示在 MLflow Traces 中
- 支持搜索和篩選功能

現在您可以在每次 completion 調用時自動收集和記錄用戶信息、會話信息等元數據，這些信息會被 MLflow 正確追蹤和存儲。

---

